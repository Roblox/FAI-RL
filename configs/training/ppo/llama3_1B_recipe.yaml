model:
  base_model_name: "meta-llama/Llama-3.2-1B-Instruct"
  value_model_name: "meta-llama/Llama-3.2-1B-Instruct"
  torch_dtype: "bfloat16"
  low_cpu_mem_usage: true
  load_in_8bit: false
  load_in_4bit: false

data:
  datasets: # YOUR TRAINING DATASET
    - name: "trl-internal-testing/descriptiveness-sentiment-trl-style"
      split: "descriptiveness"
      prompt_column: "prompt"
      chosen_column: "chosen"
      rejected_column: "rejected"
  max_length: 2048
  max_prompt_length: 1024
  remove_unused_columns: false

training:
  algorithm: "ppo"
  output_dir: "models/YOUR_MODEL_OUTPUT"
  
  # Training hyperparameters
  per_device_train_batch_size: 1
  gradient_accumulation_steps: 16
  learning_rate: 1.0e-6
  num_train_epochs: 3
  max_steps: 10000
  
  # PPO specific parameters
  gamma: 1.0                                # Discount factor
  lam: 0.95                                 # GAE lambda
  cliprange: 0.2                            # PPO clipping range
  cliprange_value: 0.2                      # Value function clipping range
  vf_coef: 0.1                              # Value function coefficient
  
  # Logging and checkpointing
  logging_steps: 5
  save_steps: 100
  
  # Optimization settings
  bf16: true
  gradient_checkpointing: true

wandb:
  enabled: true
  project: "YOUR_PROJECT"
  entity: "YOUR_ENTITY"
  name: "YOUR_NAME"
  tags: ["PPO", "llama3", "1B-Inst", "GSM8K", "OpenMathInstruct-2"]
