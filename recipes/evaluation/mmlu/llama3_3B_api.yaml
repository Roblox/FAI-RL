# MMLU Evaluation Configuration for API-based Inference
# This config evaluates a model via API (e.g., OpenAI, Google, Anthropic, hosted LLM) on the MMLU benchmark
# using automatic JSON answer extraction and accuracy calculation.
# 
# Supported Providers (automatically detected from api_endpoint URL):
# - OpenAI/ChatGPT: Detects "api.openai.com" in endpoint
# - Google/Gemini: Detects "generativelanguage.googleapis.com" in endpoint
# - Anthropic/Claude: Detects "api.anthropic.com" in endpoint
# - Custom/Other: Any other endpoint (uses generic API format)
#
# Examples:
# 1. OpenAI ChatGPT:
#    api_endpoint: "https://api.openai.com/v1/chat/completions"
#    api_key: "sk-..."  # Your OpenAI API key
#    model: "gpt-4"  # or "gpt-3.5-turbo", "gpt-4o", etc.
#
# 2. Google Gemini:
#    api_endpoint: "https://generativelanguage.googleapis.com/v1/models/gemini-pro:generateContent"
#    api_key: "AIza..."  # Your Google API key
#    model: "gemini-pro"  # or "gemini-1.5-pro", etc.
#
# 3. Anthropic Claude:
#    api_endpoint: "https://api.anthropic.com/v1/messages"
#    api_key: "sk-ant-..."  # Your Anthropic API key
#    model: "claude-3-5-sonnet-20241022"  # or "claude-3-opus-20240229", etc.

evaluation:
  # API Configuration
  # For API-based inference, provide both model identifier and API credentials
  api_endpoint: "https://<YOUR_API_ENDPOINT>"  # Custom API endpoint URL for hosted model
  api_key: "<YOUR_API_KEY>"                    # API authentication key (e.g., OpenAI API key)
  model: "meta-llama/Llama-3.2-3B-Instruct"    # API model identifier (format depends on API provider)
  
  # Where to save evaluation results (CSV format with all dataset columns + accuracy)
  output_file: "outputs/qwen3-4B-inst-dpo-full-v1-eval-mmlu-ckpt100-api.csv"
  

  # Dataset Configuration
  # Specifies which dataset and subset to evaluate on
  dataset_name: "cais/mmlu"                  # HuggingFace dataset identifier
  dataset_subset: "all"                      # Specific subset of the dataset (optional). Available: 57 subjects (e.g., "abstract_algebra", "college_biology", "high_school_physics"), or "all" for all subjects
  dataset_split: "test"                      # Which split to evaluate on: test, validation, or dev
  dataset_columns: ["question", "choices", "answer"]  # List of dataset columns to include in evaluation
  ground_truth_column: "answer"              # Column containing the correct answers for accuracy calculation
  
  # System Prompt Template
  # Template for evaluation prompts (supports variable substitution with {variable})
  # The model will be prompted with this template for each evaluation sample
  system_prompt: |
    Question: {question}
    Choose the best option and respond only with the letter of your choice.
    
    {choices}
    
    Please respond **only in valid JSON format** with the following keys:
    {{
      "answer": "<the letter of the chosen option, e.g., A, B, C, D>"
    }}

    Let's think step by step.
  
  # Generation Parameters
  # Controls how the model generates responses during evaluation
  temperature: 1.0                           # Sampling temperature for response generation (0.0 = deterministic, 2.0 = very random)
  top_p: 0.9                                 # Nucleus sampling parameter - probability threshold for token selection (0.0-1.0, lower = more focused)
  max_new_tokens: 2000                       # Maximum tokens to generate per response
  do_sample: true                            # Whether to use sampling for generation (false = greedy decoding, true = stochastic sampling)
