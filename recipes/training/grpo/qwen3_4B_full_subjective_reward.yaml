# Model Configuration

# Reward API Configuration for Dynamic Evaluation (Optional)
reward_api:
  endpoint: "https://<YOUR_API_ENDPOINT>"  # Custom API endpoint URL for hosted model
  key: "<YOUR_API_KEY>"                    # API authentication key (e.g., OpenAI API key)
  model: "<YOUR_MODEL_IDENTIFIER>"         # API model identifier (format depends on API provider)

# Model Configuration
# Defines the base model and its loading parameters
model:
  base_model_name: "Qwen/Qwen3-4B-Instruct-2507"        # HuggingFace model name
  torch_dtype: "bfloat16"                               # Data type for model weights (bfloat16/float16/float32)
  low_cpu_mem_usage: true                               # Reduce CPU memory usage during model loading
  load_in_8bit: false                                   # Enable 8-bit quantization
  load_in_4bit: false                                   # Enable 4-bit quantization (for QLoRA)
  use_flash_attention: false                            # Use Flash Attention for faster training (if supported)

# Data Configuration
# Specifies datasets and preprocessing settings
data:
  datasets: # YOUR TRAINING DATASET
    # Supports multiple datasets.
    # Add additional entries here for combined training across datasets.
    - name: "Roblox/FAI-RL-150K-training-dataset"
      split: "train"
      prompt_column: "prompt"  # Column name containing the prompts
  
  # Text processing settings
  max_length: 2048                                      # Maximum sequence length for model input
  max_prompt_length: 1024                               # Maximum length for prompts (rest reserved for responses)
  remove_unused_columns: false                          # Keep all dataset columns (set true to save memory)

# Training Configuration
# Controls the training process and optimization settings
training:
  algorithm: "grpo"                                     # Training algorithm: sft, dpo, ppo, grpo, gspo
  output_dir: "models/qwen3_4B_Inst_GRPO_full_v1"       # Directory to save trained model and checkpoints
  
  # Core training hyperparameters
  per_device_train_batch_size: 1                        # Batch size per GPU (adjust based on GPU memory)
  gradient_accumulation_steps: 16                       # Steps to accumulate gradients (effective batch = batch_size × accum_steps × num_gpus)
  learning_rate: 1.0e-5                                 # Learning rate (1e-5 for full fine-tuning)
  num_train_epochs: 1                                   # Number of complete passes through the dataset
  max_steps: -1                                         # Maximum number of training steps (-1 = train for num_train_epochs)
  warmup_steps: 50                                      # Linear warmup steps for learning rate scheduler
  
  # Logging and checkpointing
  logging_steps: 5                                      # Log training metrics every N steps
  save_steps: 100                                       # Save model checkpoint every N steps
  eval_steps: 100                                       # Evaluate model every N steps (optional, used by DPO/GRPO/GSPO/SFT)
  
  # Memory and precision optimization
  bf16: true                                            # Use bfloat16 precision (recommended for modern GPUs)
  fp16: false                                           # Use float16 precision (alternative to bf16)
  gradient_checkpointing: true                          # Trade compute for memory (enables larger models/batch sizes)
  
  # Data loading optimization
  dataloader_num_workers: 0                             # Number of CPU workers for data loading (0 = main process only)
  dataloader_pin_memory: false                          # Pin memory for faster GPU transfer (set true if sufficient RAM)
  dataloader_drop_last: true                            # Drop last incomplete batch to ensure consistent batch sizes
  
  # Output and evaluation settings
  save_only_model: true                                 # Save only model weights (not optimizer states) to reduce disk usage
  prediction_loss_only: true                            # Only compute prediction loss during evaluation

  # GRPO-specific parameters
  num_generations: 8                                    # Group size for sequence grouping

# Weights & Biases Integration
# Optional experiment tracking and monitoring
wandb:
  enabled: false                                        # Enable W&B logging
  project: "rl"                                         # W&B project name
  entity: "fai-llmaas"                                  # W&B username or team name
  name: "Qwen3-4B-Inst-Full-GRPO"                       # Experiment name in W&B
  tags: ["GRPO", "llama3", "3B-Inst", "FAI-RL-dataset"]  # Tags for organizing experiments
