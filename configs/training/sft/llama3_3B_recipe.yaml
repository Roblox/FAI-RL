model:
  base_model_name: "meta-llama/Llama-3.2-3B-Instruct"
  torch_dtype: "bfloat16"
  low_cpu_mem_usage: true
  load_in_8bit: false
  load_in_4bit: false
  use_flash_attention: false

data:
  datasets: # YOUR TRAINING DATASET
    - name: "yahma/alpaca-cleaned"
      split: "train"
      prompt_column: "instruction"
      answer_column: "output"
  max_length: 2048
  max_prompt_length: 1024
  remove_unused_columns: false

training:
  algorithm: "sft"
  output_dir: "models/YOUR_MODEL_OUTPUT"
  run_name: "UNIQUE_ID"
  
  # Training hyperparameters
  per_device_train_batch_size: 1
  gradient_accumulation_steps: 16
  learning_rate: 2.0e-5
  num_train_epochs: 3
  max_steps: -1
  warmup_steps: 100
  
  # Logging and checkpointing
  logging_steps: 5
  save_steps: 500
  eval_steps: 500
  
  # Optimization settings
  bf16: true
  fp16: false
  gradient_checkpointing: true
  
  # DataLoader settings
  dataloader_num_workers: 0
  dataloader_pin_memory: false
  dataloader_drop_last: true


wandb:
  enabled: true
  project: "rl"
  entity: "fai-llmaas"
  name: "Llama-3.1-3B-Inst-OSS-SFT-FAIRL"
  tags: ["SFT", "llama3", "3B-Inst", "OSS"]