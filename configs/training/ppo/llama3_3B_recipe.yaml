model:
  base_model_name: "meta-llama/Llama-3.2-3B-Instruct"
  value_model_name: "EleutherAI/pythia-160m"
  torch_dtype: "bfloat16"
  low_cpu_mem_usage: true
  load_in_8bit: false
  load_in_4bit: false
  use_flash_attention: false

data:
  datasets: # YOUR TRAINING DATASET
    - name: "openai/gsm8k"
      subset: "main"
      split: "train"
      prompt_column: "question"
      answer_column: "answer"
    - name: "nvidia/OpenMathInstruct-2"
      split: "train_1M"
      prompt_column: "problem"
      answer_column: "expected_answer"
  max_length: 2048
  max_prompt_length: 1024
  remove_unused_columns: false

training:
  algorithm: "ppo"
  output_dir: "models/YOUR_MODEL_OUTPUT"
  run_name: "UNIQUE_ID"
  
  # Training hyperparameters
  per_device_train_batch_size: 1
  gradient_accumulation_steps: 16
  learning_rate: 1.0e-6
  num_train_epochs: 3
  max_steps: -1
  warmup_steps: 50
  
  # PPO specific parameters
  ppo_epochs: 4                             # Number of PPO epochs per batch
  init_kl_coef: 0.2                         # Initial KL coefficient
  target_kl: 6.0                            # Target KL divergence
  adap_kl_ctrl: true                        # Adaptive KL control
  gamma: 1.0                                # Discount factor
  lam: 0.95                                 # GAE lambda
  cliprange: 0.2                            # PPO clipping range
  cliprange_value: 0.2                      # Value function clipping range
  vf_coef: 0.1                              # Value function coefficient
  max_grad_norm: 1.0                        # Max gradient norm for clipping
  temperature: 1.0                          # Sampling temperature
  top_k: 0                                  # Top-k sampling (0 means no top-k)
  top_p: 1.0                                # Top-p sampling
  do_sample: true                           # Whether to sample
  use_score_scaling: false                  # Whether to scale scores
  use_score_norm: false                     # Whether to normalize scores
  score_clip: null                          # Score clipping threshold
  seed: 0                                   # Random seed
  
  # Logging and checkpointing
  logging_steps: 5
  save_steps: 100
  eval_steps: 100
  
  # Optimization settings
  bf16: true
  fp16: false
  gradient_checkpointing: true
  
  # DataLoader settings
  dataloader_num_workers: 0
  dataloader_pin_memory: false
  dataloader_drop_last: true
  
  # Miscellaneous
  save_only_model: true
  prediction_loss_only: true

wandb:
  enabled: true
  project: "YOUR_PROJECT"
  entity: "YOUR_ENTITY"
  name: "YOUR_NAME"
  tags: ["PPO", "llama3", "3B-Inst", "GSM8K", "OpenMathInstruct-2"]
